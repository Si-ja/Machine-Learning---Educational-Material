{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines in Machine Learning\n",
    "\n",
    "This is a very quick notebook presenting some information on how to utilize pipelines in machine learning processes and what are they in general.\n",
    "\n",
    "* Sections:\n",
    "    - What are Pipelines\n",
    "    - Why they can matter\n",
    "    - Practical example\n",
    "    - Further\n",
    "\n",
    "To follow along with the notebook, please take a look at what dependenices are being used in it (presented in the following block of code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python 3.7.3 \n",
      "\n",
      "With the following packages:\n",
      "------------------------------\n",
      "Pandas: 0.25.0\n",
      "Sklearn: 0.21.2\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "# Data Processing\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Pipeline and parameters selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Module to save and load our models\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# NOT REQUIRED: Some extra, to allow for errors surpressing in the notebook and checking for the python version\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Using Python {sys.version.split('(')[0]}\\n\")\n",
    "print(\"With the following packages:\")\n",
    "print(30 * \"-\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Sklearn: {sklearn.__version__}\")\n",
    "print(30 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install them via pip, or conda. The commands for that are (for the versions used in the notebook, or above):\n",
    "\n",
    "* For Pandas: \n",
    "```pip install pandas>=0.25.0; conda install pandas>=0.25.0```\n",
    "\n",
    "* For Sklearn:\n",
    "```pip install scikit-learn>=0.21.2; conda install scikit-learn>=0.21.2```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Pipelines\n",
    "\n",
    "Pipelines are essentially what they sound to be - a process in which some actions are lined up into a pattern that is executed every single time in the same way. The concept of this might seem redundant, as if we want to use the principle of building pipelines with code - then it would be fair to say that we have always been doing that. This is because code usually needs to be executed in a particular sequence every single time before one can derive to the desired result.\n",
    "\n",
    "However, pipelines in the context of Machine Learning and more specifically this tutorial, are not general concepts, but rather already created classes from other packages that can be utilized in a way, that allow for less coding and faster production deployment of prepared models for various predictions/classification/forecasting/etc. tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why they can matter\n",
    "\n",
    "Theory alone can be quite dry, but it is import to understand what they can do, before diving into making them and evaluating their actual benefit.\n",
    "\n",
    "Let's break a Machine Learning tasks into steps in to as basic of a form as we can:\n",
    "\n",
    "* Step 1:\n",
    "    - Gather and clean data (We assume we already have it)\n",
    "    - Split data into training and testing samples (or even into train-validate-test)\n",
    "    \n",
    "* Step 2:\n",
    "    - Prepare a Machine Learning Model with leading steps to it (Potentially in the following way):\n",
    "        - Transform/Scale the data\n",
    "        - Perform dimensionality reduction\n",
    "        - Train a classifier, potentially evaluating which hyperparameters work best through Grid Search\n",
    "    - Evaluate the Model\n",
    "    - Alter the Model if required\n",
    "    - Save the Model\n",
    "\n",
    "* Step 3:\n",
    "    - Deploy the Model into production    \n",
    "    \n",
    "Step 1 is a story on it's own, but consider than when you have the data - that step is over and you can move to the next one. Step 2 that matters to us - is a bit messy. It essentially requires to create a sequential data processing pipeline. Data moves from one pre-processing step to another and even though it might be good to have more control over certain processes, by programming them on the spot and debugging, real problems might start at Step 3, where every single model crucial step needs to be repeated for it to work in a desired way. This is where the pre-existing pipelines can come in. By creating a pipeline in the Step 2, you would just need to transport it into the deployment stage further, rather than being required to re-write the whole data analyzing process, with a trained model in it.\n",
    "\n",
    "If it still sounds a bit confussing, don't worry, a real application might help clear some things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical example\n",
    "\n",
    "Practical example will not be too complex, but enough to get a general sense of what is going on with pipelines. For it, a very standard Iris database will be used. Let's perform the first step into getting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Load the data (iris) -----\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# To X assign data that describes some flower's attributes \n",
    "X = iris.data   \n",
    "\n",
    "# To y assign data that tells which flower is in records\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the loaded information for IV\n",
    "pd.DataFrame(X, columns = iris.feature_names).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flower Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Flower Group\n",
       "0             0\n",
       "1             0\n",
       "2             0\n",
       "3             0\n",
       "4             0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the loaded information for DV\n",
    "pd.DataFrame(y, columns = [\"Flower Group\"]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working in the real world - a good approach would be to actually analyze your data and learn something about it. Something of the following type could be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the following amount of flowers in each group, classified as 0, 1, 2: {0: 50, 1: 50, 2: 50}\n"
     ]
    }
   ],
   "source": [
    "flowers = {}\n",
    "for value in y:\n",
    "    if value not in flowers:\n",
    "        flowers[value] = 1\n",
    "    else:\n",
    "        flowers[value] += 1\n",
    "        \n",
    "print(f\"We have the following amount of flowers in each group, classified as 0, 1, 2: {flowers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even more with graphical analysis, using libraries like seaborn and matplotlib. But Iris dataset is one of the more basic ones, and it feels a bit of a waste of time to spend too much time on it. Therefore, this step is quickly glanced over, though in the real life - it can be as important and crucial of addressing, as anything done in the whole project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider that we have all the data we need for the Step 2 by now. Therefore, we can split it into the trainig and testing sets. We can do it easily with the following approach, by dedicating 15% of the data to testing and the rest for training. An important note - the following approach will also shuffle the data. As well, our data is properly distributed, so we do not need to worry about some samples being too bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Split the data -----\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move to step 2 and create a pipeline. Let's say we want to first scale our data (standardize features by removing the mean and scaling to unit variance), then applying a PCA (essentially reducing the data dimensionality) and then classifying our data, with a Decision Tree Classifier that we will train. This can be done in multiple steps, by calling a number of packages and creating multiple objects for those procedures. Sequentially calling methods from those objects to transform our data and then working with the inputs and outputs of each to pass to the next object, until we reach our goal.\n",
    "\n",
    "Luckily, this tutorial is here to resolve this inconvenience. Using the `sklearn.pipeline.Pipeline` class we can do that in a much simpler fashion. The object of this class takes a list of parameters. Those parameters - are classes that can be utilized in transforming our data in a sequential manner. Each parameter needs to be passed in a tuple, where the first argument is the name of the data transformer that we can assign as we want, and the second - the class that will take care of our data transformation requirement. Data transforming classes can be also custom, but they will have to be made in the same matter as most of sklearn ones.\n",
    "\n",
    "In the following example a pipeline is created, and data we have is fitted with it, essentially allowing for training of the model to happen in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=None,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('dt',\n",
       "                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features=None,\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort=False, random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Build a pipeline -----\n",
    "pipe = Pipeline([('scaler', StandardScaler()),        # Scale the data\n",
    "                 ('pca', PCA()),                      # Apply a PCA\n",
    "                 ('dt', DecisionTreeClassifier())],   # Run a Decision Tree Classifier\n",
    "                 verbose = 0)              \n",
    "# Apply the data for training\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score: 95.65%\n"
     ]
    }
   ],
   "source": [
    "# ----- See how well our default learner performs -----\n",
    "print(f\"Current score: {round(100*pipe.score(X_test, y_test), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline holds a classifier, than can classify data with quite a fair accuracy. But there is a problem. We have not set a single hyperparameter and used everything in our pipeline as a default set up. Luckily, we can perform a gird search and see, which hyperparameters fit best for each of our classes in the pipeline.\n",
    "\n",
    "Let's create a dictionary, that will allow us to do that.\n",
    "\n",
    "The first value in the dictionary needs to be the name of the parameter that we are setting. It needs to be called in the following way: \"the name to the class that we have assigned in our pipeline\" + \"\\_\\_\" + \"the hyperparameter name the class uses\". Next to it, a list should be created of hyperparameters we want to experiment with. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Implement a Grid Search to test more parameters and find best ones -----\n",
    "param_grid = {\n",
    "    \"pca__n_components\": [1, 2, 3, 4],\n",
    "    \"dt__criterion\": [\"gini\", \"entropy\"],      # default: gini\n",
    "    \"dt__max_depth\": [1, 2, 3, 4, 5]           # default: None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not know the name of the parameters you can experiment with, either __a)__ look into the documentation of those classes that allow for data transformation, or __b)__ call the \"Class..get_params().keys()\" command, which will display all the parameters you can modify. Though with the second approach you still might miss information on what type of variables those parameters take. Consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['copy', 'iterated_power', 'n_components', 'random_state', 'svd_solver', 'tol', 'whiten'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Parameters to modify for the PCA procedure -----\n",
    "PCA().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'presort', 'random_state', 'splitter'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Parameters to modify for the Decision Tree Classifier -----\n",
    "DecisionTreeClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have prepared your set of parameters to experiment with, they can be assigned into a GridSearchCV class together with the pipeline and called for execution and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best parameter (CV score = 96.06%)\n",
      "Best Parameters for the job: {'dt__criterion': 'gini', 'dt__max_depth': 5, 'pca__n_components': 3}\n"
     ]
    }
   ],
   "source": [
    "search = GridSearchCV(pipe, param_grid)\n",
    "# ----- Initiate the Search and Check Results -----\n",
    "search.fit(X_train, y_train)\n",
    "print(\"\\n\")\n",
    "print(f\"Best parameter (CV score = {round(100*search.best_score_, 2)}%)\")\n",
    "print(f\"Best Parameters for the job: {search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With optimal hyperparamet settings found we can consider that the prediction power of our model in the pipeline could increase, if we set them. \n",
    "\n",
    "Now, the whole pipeline can be saved with optimized settings (unless we want to check for more things to work on) and deployed into production. To do that, we can either pickle the model, or use the sklearn saving methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_classifier_pipeline.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Save the model with optimal parameters under the name of iris_classifier_pipeline ----- \n",
    "joblib.dump(search.best_estimator_, \"iris_classifier_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Load the previously saved model that can be deployed -----\n",
    "loaded_pipeline = joblib.load(\"iris_classifier_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=3,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('dt',\n",
       "                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n",
       "                                        max_depth=5, max_features=None,\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort=False, random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Evaluate if it's the model with optimal parameters -----\n",
    "loaded_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model is: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# ----- Use the loaded model to predict unseen previously data -----\n",
    "predictions = loaded_pipeline.predict(X_test)\n",
    "\n",
    "# ----- Compare those predictions to the reality of things -----\n",
    "answers = (loaded_pipeline.predict(X_test) == y_test)\n",
    "\n",
    "# ----- Evaluate how many answers were correct -----\n",
    "accuracy = round(100 * (sum(answers) / len(answers)), 2)\n",
    "print(f\"Accuracy of our model is: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy actually increased compared to the default one, but do not forget to consider that during the grid search only the training data was evaluated. And though in both cases models performed almost equally well, with the testing set; in reality, with a bigger data corpus the second model is likely to be much better in the long run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further\n",
    "\n",
    "What this tutorial does not cosider, is cases when datasets are really huge and a need for data loaders might be present (something that would gradually load data into the pipeline, for both training and predictions) as well as how custom data transforming classes could be developed. This is something that I wanted to keep out, to not make the tutorial confussing and make one of a more simple nature. But those aspects are definitelly something that are worth while to be considered in the future.\n",
    "\n",
    "For now - I wanted to make something to share and for personal learning, as I noticed I've never actually worked with such approach to pipelines, that can in fact be quite beneficial and convenient for at least small scale applications. Bigger ones might in fact require a more sophisticated set of tools and skills."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('PyTorch2019': conda)",
   "language": "python",
   "name": "python37364bitpytorch2019conda6b5071966ab649f39402bb9e20bc54dd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
